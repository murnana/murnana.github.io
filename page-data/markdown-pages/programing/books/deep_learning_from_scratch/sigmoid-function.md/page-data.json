{"componentChunkName":"component---src-templates-markdown-pages-template-js","path":"/markdown-pages/programing/books/deep_learning_from_scratch/sigmoid-function.md","result":{"data":{"markdownRemark":{"html":"<p>入力信号の総和を出力信号に変換する関数を、一般に <strong>活性化関数</strong>\n(activation function)という。\n活性化関数には候補となる関数がいくつかある。</p>\n<p>パーセプトロンの場合は、閾値を境にして値が変わるため、「ステップ関数」や「階段関数」と呼ばれる</p>\n<p>活性化関数はほかにもある。 今回は <strong>シグモイド関数</strong> (sigmoid\nfunction)について。</p>\n<h1>シグモイド関数</h1>\n<p>シグモイド関数の一般項は</p>\n<p>$$h(x) = \\frac{1 + \\exp(-x)}{1}$$</p>\n<p>ここでいう exp とは <strong>ネイピア数</strong> のことを指す。</p>\n<p>::: {.plot}\nimport numpy import matplotlib.pylab as plt</p>\n<p>\"\"\" ステップ関数 :param numpy.array: x \"\"\" def step_function(x):\ny = x > 0 # 配列 X の各要素を比較した結果の配列を Y に入れる return\ny.astype(numpy.int) # Y の各要素 boolean を int がたに変換</p>\n<p>\"\"\" シグモイド関数 :param numpy.array: x \"\"\" def sigmoid(x):\nreturn 1 / (1 + numpy.exp(-x)) # exp = 指数関数のこと</p>\n<p>if __name__ == \"__main__\":</p>\n<p>: x = numpy.arange(-5.0, 5.0, 0.1) # 0.1 刻みで、-5.0 ~ 5.0 の配列 y1\n= step_function(x) y2 = sigmoid(x) plt.plot(x, y1, linestyle =\n'--', label = 'step') plt.plot(x, y2, label = 'sigmoid')\nplt.ylim(-0.1, 1.1) # y 軸の範囲 plt.show()\n:::</p>\n<h1>Rectified Linear Unit; ReLU (正規化線形関数、ランプ関数)</h1>\n<p>ReLU は、 <strong>0 未満の時は 0、0 以上の時は 0 以上の値</strong> を出力する関数</p>\n<p>$$\nh(x) = \\left{ \\begin{array}{ll}\nx &#x26; (x > 0) \\\n0 &#x26; (x \\leq 0)\n\\end{array} \\right.\n\\end{aligned}$$</p>\n<pre><code class=\"language-{.python}\">def relu(x):\n    return np.maximum(0, x)\n</code></pre>\n<h1>3層ニューラルネットワークの実装</h1>\n<p>行列とは何かは知っているので省略！</p>\n<pre><code class=\"language-{.python}\">import numpy\nimport matplotlib.pylab as plt\nimport step_and_sigmoid\n\n\n\"\"\"\n恒常関数\n値をそのまま返す関数のこと\n:param numpy.array: x\n\"\"\"\ndef identity_function(x):\n    return x\n\n\n\n\"\"\"\nネットワークの構築\n重みとバイアスの初期化をします\n…本当は学習結果で分かった重みを突っ込む気がする\n\"\"\"\ndef init_network():\n    network = {}\n    # 1層目の重みとバイアス\n    network['W1'] = numpy.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n    network['b1'] = numpy.array([0.1, 0.2, 0.3])\n\n    # 2層目の重みとバイアス\n    network['W2'] = numpy.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n    network['b2'] = numpy.array([0.1, 0.2])\n\n    # 3層目の重みとバイアス\n    network['W3'] = numpy.array([[0.1, 0.3], [0.2, 0.4]])\n    network['b3'] = numpy.array([0.1, 0.2])\n\n    return network\n\n\n\"\"\"\n入力からニューラルネットワークを通じて出力する\n\"\"\"\ndef forward(network, x):\n    # 1層目のニューロンを計算する\n    a1 = numpy.dot(x, network['W1']) + network['b1']\n    z1 = step_and_sigmoid.sigmoid(a1)\n\n    # 2層目のニューロンを計算する\n    a2 = numpy.dot(z1, network['W2']) + network['b2']\n    z2 = step_and_sigmoid.sigmoid(a2)\n\n    # 3層目のニューロンを計算する\n    a3 = numpy.dot(z2, network['W3']) + network['b3']\n    y = identity_function(a3)\n\n    return y\n\n\nif __name__ == \"__main__\":\n    network = init_network()\n    print(f'network is {network}')\n\n    x = numpy.array([1.0, 0.5])\n    print(f'x = {x}')\n\n    y = forward(network, x)\n    print(f'y = {y}')\n</code></pre>\n<h1>学習問題の種類</h1>\n<p>補足と言いつつ実は大事なのでは</p>\n<h2>分類問題</h2>\n<p>データがどのクラスに属するのか。\n例えば、人の画像から性別を判断する、など</p>\n<p>-</p>\n<pre><code>ソフトマックス\n\n:   全ての入力信号から影響を受ける出力信号を1つ出す\n    出力は合計すると1になることから、「確率」として解釈することができる\n</code></pre>\n<h2>回帰問題</h2>\n<p>データから(連続的な)数値を予測する問題。\n例えば、人の画像から体重を予測する、など</p>\n<p>-</p>\n<pre><code>恒常関数\n\n:   入力信号1つに対して、出力信号を1つ出す\n</code></pre>\n<h1>指数(exp)の扱いに注意</h1>\n<p>指数をそのまま扱うと、オーバーフローで :py<code>infinity</code>{.interpreted-text\nrole=\"const\"} や ::py<code>nan</code>{.interpreted-text role=\"const\"}\nが出てきたりする\nPython2かPython3かによっても最大値が変わるので、都度確認したほうが良い。</p>\n<p>今回はPython3なので...</p>\n<ul>\n<li>整数は(一応)無制限</li>\n<li>浮動小数点はCのdouble($2.225074 * 10^{-308} &#x3C; x &#x3C; 1.797693 * 10^{308}$)</li>\n</ul>\n<p>数値の型はほかにもあるけれど割合...</p>\n<p>::: {.seealso}\n<a href=\"http://enajet.air-nifty.com/blog/2011/09/python-9a0e-1.html\">Python ：　整数最大値、辞書項目数の最大値:\nenajet</a></p>\n<p><a href=\"https://docs.python.org/ja/3/library/stdtypes.html#typesnumeric\">4. 組み込み型 --- Python 3.6.5 ドキュメント</a></p>\n<p>:   4.4. 数値型 int, float, complex</p>\n<p><a href=\"http://www.cc.kyoto-su.ac.jp/~yamada/programming/float.html\">浮動小数点数型と誤差</a>\n:::</p>\n<h1>出力層のニューロン数</h1>\n<p>出力層は一般的に、分類したい種類(クラス)の数に設定する</p>\n<h1>MNIST を使用した実践</h1>\n<p>Python3系ではPILは使えない(おかしい、Python3の書籍のはず...)\nので、pillowを入れる。</p>\n<pre><code class=\"language-{.shell}\">$ pip install pillow\n</code></pre>\n<p>このドキュメントではpipenvを使っているので</p>\n<pre><code class=\"language-{.shell}\">$ pipenv install pillow\n</code></pre>\n<p>::: {.seealso}\n<a href=\"https://qiita.com/ukwksk/items/483d1b9e525667b77187\">python3系でのPython Image\nLibraryの使用方法</a>\n:::\n$$</p>","frontmatter":{"title":"ニューラルネットワーク - シグモイド関数"}}},"pageContext":{"relativePath":"markdown-pages/programing/books/deep_learning_from_scratch/sigmoid-function.md"}}}